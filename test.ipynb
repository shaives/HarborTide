{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "import requests\n",
    "\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elevation(lat, lng):\n",
    "    url = f\"https://api.open-elevation.com/api/v1/lookup?locations={lat},{lng}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if 'results' in data and data['results']:\n",
    "        return data['results'][0]['elevation']\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elevation: 198.0 meters\n"
     ]
    }
   ],
   "source": [
    "elev = get_elevation(21.3866284869, -157.905641308)\n",
    "print(\"Elevation:\", elev, \"meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import_bases(states = [\"Alaska\", \"Alabama\", \"Arkansas\", \"American Samoa\", \"Arizona\", \"California\", \"Colorado\", \"Connecticut\", \"District \", \"of Columbia\", \"Delaware\", \"Florida\", \"Georgia\", \"Guam\", \"Hawaii\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Massachusetts\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Puerto Rico\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Virginia\", \"Virgin Islands\", \"Vermont\", \"Washington\", \"Wisconsin\", \"West Virginia\", \"Wyoming\"]):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a cleaned dataframe with all the military installations.\n",
    "\n",
    "            Parameters:\n",
    "                        states (list): list of states you want to see bases off\n",
    "\n",
    "            Data:\n",
    "                        https://public.opendatasoft.com/explore/dataset/military-bases/export/?dataChart=eyJxdWVyaWVzIjpbeyJjb25maWciOnsiZGF0YXNldCI6Im1pbGl0YXJ5LWJhc2VzIiwib3B0aW9ucyI6e319LCJjaGFydHMiOlt7ImFsaWduTW9udGgiOnRydWUsInR5cGUiOiJjb2x1bW4iLCJmdW5jIjoiQVZHIiwieUF4aXMiOiJvYmplY3RpZF8xIiwic2NpZW50aWZpY0Rpc3BsYXkiOnRydWUsImNvbG9yIjoiI0ZGNTE1QSJ9XSwieEF4aXMiOiJjb21wb25lbnQiLCJtYXhwb2ludHMiOjUwLCJzb3J0IjoiIn1dLCJ0aW1lc2NhbGUiOiIiLCJkaXNwbGF5TGVnZW5kIjp0cnVlLCJhbGlnbk1vbnRoIjp0cnVlfQ%3D%3D&location=6,37.78808,-111.09375&basemap=jawg.light\n",
    "\n",
    "            Returns:\n",
    "                        bases_df (DataFrame): \n",
    "    \"\"\"\n",
    "\n",
    "    # Choose columns we need\n",
    "    data_columns = ['Geo Point', 'COMPONENT', 'Site Name', 'State Terr', 'Oper Stat']\n",
    "\n",
    "    # read in alle the military bases - seperator ';'\n",
    "    bases_df = pd.read_csv('./data/military-bases.csv', sep=';', usecols = data_columns )\n",
    "\n",
    "    # rename columns for easier use\n",
    "    bases_df.rename(columns={'Geo Point': 'geoPoint', 'COMPONENT': \"component\", 'Site Name': 'name', 'State Terr': 'state', 'Oper Stat': 'status'}, inplace = True)\n",
    "\n",
    "    # checking it there is only Active and Inaktive\n",
    "    #bases_df[(bases_df.status != 'Active') & (bases_df.status != 'Inactive')]\n",
    "\n",
    "    # drop all Inaktive bases\n",
    "    bases_df = bases_df[bases_df.status != 'Inactive']\n",
    "\n",
    "    # convert geo points\n",
    "    bases_df[['lat', 'lon']] = bases_df.geoPoint.str.split(',', expand = True).astype('float64')\n",
    "    bases_df.geoPoint = list(zip(bases_df.lat, bases_df.lon))\n",
    "    bases_df.drop(columns = ['lat', 'lon'], inplace =  True)\n",
    "\n",
    "    # filter by list of states\n",
    "    bases_df = bases_df[bases_df.state.isin(states)]\n",
    "\n",
    "    # get elevation for each location\n",
    "    # need to be improved with POST request\n",
    "    bases_df['elevation'] = bases_df['geoPoint'].apply(lambda point: get_elevation(point[0], point[1]))\n",
    "\n",
    "    return bases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import_tidel_sensors():\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a dictonary with the information about the sensor and the data.\n",
    "\n",
    "            Parameters:\n",
    "                        None\n",
    "\n",
    "            Returns:\n",
    "                        sensors (dict): dictionary consisting out of:\n",
    "\n",
    "                            Info    (dict): Metadata of the Sensor\n",
    "\n",
    "                                e.g.\n",
    "                                NOS ID: 9410170\n",
    "                                Location Name: SAN DIEGO, SAN DIEGO BAY\n",
    "                                Latitude: 32.71419\n",
    "                                Longitude: -117.17358\n",
    "                                Horizontal Datum: WGS-84\n",
    "                                Operator: DOC>NOAA>NOS>CO-OPS\n",
    "                                Vertical Datum: Station Datum\n",
    "\n",
    "                            Data    (dict): Data the sensor collected\n",
    "\n",
    "            Source:\n",
    "                        https://www.ngdc.noaa.gov/hazard/tide/\n",
    "\n",
    "    \"\"\"\n",
    "    sensor_information_dict = dict()\n",
    "    csv_header = list()\n",
    "    files_in_directory_gz = list()\n",
    "    sensor_data_df = pd.DataFrame()\n",
    "\n",
    "    # Getting all the files in the directory\n",
    "    files_in_directory = os.listdir('./data/tide_sensors')\n",
    "\n",
    "    # Getting all the gz files\n",
    "    for file in files_in_directory:\n",
    "        \n",
    "        # Checking if the file is a gz file\n",
    "        if file.endswith('.gz'):\n",
    "            \n",
    "            # Appending the file to the list\n",
    "            files_in_directory_gz.append(file)\n",
    "\n",
    "    # Looping through all the files\n",
    "    for idx, file in enumerate(files_in_directory_gz):\n",
    "\n",
    "        with gzip.open('./data/tide_sensors/' + file, 'rt') as file_in:\n",
    "            \n",
    "            # Reading in header\n",
    "            sensor_information_file = file_in.readlines()[0:10]\n",
    "\n",
    "        if len(csv_header) == 0:\n",
    "            # Retrieving the header for csv\n",
    "            csv_header = sensor_information_file[-1].removeprefix('// ').removesuffix('\\n').split(',')\n",
    "            csv_header = [item.strip() for item in csv_header]\n",
    "\n",
    "        # Getting metadata for the sensor\n",
    "        for line in sensor_information_file[:-4]:\n",
    "            \n",
    "            name, value = line.removeprefix('// ').strip().split(':')\n",
    "\n",
    "            if name in sensor_information_dict:\n",
    "                sensor_information_dict[name].append(value)\n",
    "            else :\n",
    "                sensor_information_dict[name] = [value]\n",
    "        \n",
    "        sensor_data_df_temp = pd.read_csv('./data/tide_sensors/' + file, skiprows=10, sep='\\t', header=None, usecols=[0,1])\n",
    "        sensor_data_df_temp.columns = csv_header[:2]\n",
    "        sensor_data_df_temp['NOS ID'] = int(sensor_information_dict.get('NOS ID')[idx])\n",
    "        sensor_data_df_temp['datetime [ISO8601]'] = pd.to_datetime(sensor_data_df_temp['datetime [ISO8601]'], format='ISO8601')\n",
    "        sensor_data_df = pd.concat([sensor_data_df, sensor_data_df_temp])\n",
    "\n",
    "    # Creating a df from the metadata\n",
    "    sensor_information_df = pd.DataFrame(sensor_information_dict)\n",
    "\n",
    "    sensor_information_df = sensor_information_df.assign(geoPoint = list(zip(sensor_information_df.Latitude.astype('float64'), sensor_information_df.Longitude.astype('float64'))))\n",
    "    sensor_information_df.drop(columns = ['Latitude', 'Longitude'], inplace =  True)\n",
    "\n",
    "    return sensor_information_df, sensor_data_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_tide_sensor_data(tide_sensor_df):\n",
    "\n",
    "    \"\"\"\n",
    "    Curates the tide sonsor data.\n",
    "\n",
    "            Parameters:\n",
    "                        tide_sensor_df (DataFrame): Contains all read in tide sensors\n",
    "\n",
    "            Returns:\n",
    "                        curated_tide_sensor_df \n",
    "    \"\"\"\n",
    "\n",
    "    # Remove all rows with 9999\n",
    "    tide_sensor_df = tide_sensor_df[tide_sensor_df['waterlevel_quality_controlled [m]'] != 9999]\n",
    "    # find unique sensors IDs\n",
    "    tide_sensors = tide_sensor_df['NOS ID'].unique()\n",
    "\n",
    "    # Group by NOS ID and month\n",
    "    curated_tide_sensor_mean_df = tide_sensor_df.groupby(['NOS ID', pd.Grouper(key='datetime [ISO8601]', freq='W')])['waterlevel_quality_controlled [m]'].mean()\n",
    "    curated_tide_sensor_min_df = tide_sensor_df.groupby(['NOS ID', pd.Grouper(key='datetime [ISO8601]', freq='W')])['waterlevel_quality_controlled [m]'].min()\n",
    "    curated_tide_sensor_max_df = tide_sensor_df.groupby(['NOS ID', pd.Grouper(key='datetime [ISO8601]', freq='W')])['waterlevel_quality_controlled [m]'].max()\n",
    "\n",
    "    # Reset index\n",
    "    curated_tide_sensor_mean_df = curated_tide_sensor_mean_df.reset_index()\n",
    "    curated_tide_sensor_min_df = curated_tide_sensor_min_df.reset_index()\n",
    "    curated_tide_sensor_max_df = curated_tide_sensor_max_df.reset_index()\n",
    "\n",
    "    # Save Base names for chosen sensors\n",
    "    # must become dynamic!\n",
    "    base_names_for_sensors = ['MCB Hawaii, Kaneohe Bay', 'VSTA San Diego', 'MCB Camp Pendleton', 'NAVSUPDET Monterey (NPS)', 'NAVSTA Everett, Washington']\n",
    "\n",
    "    for idx, sensor in enumerate(tide_sensors):\n",
    "\n",
    "        # Get the sensor data\n",
    "        sensor_data = curated_tide_sensor_mean_df[curated_tide_sensor_mean_df['NOS ID'] == sensor]\n",
    "\n",
    "        # Get the min and max\n",
    "        sensor_data_min = curated_tide_sensor_min_df[curated_tide_sensor_min_df['NOS ID'] == sensor]\n",
    "        sensor_data_max = curated_tide_sensor_max_df[curated_tide_sensor_max_df['NOS ID'] == sensor]\n",
    "\n",
    "        # Merge the data\n",
    "        sensor_data = sensor_data.merge(sensor_data_min[['datetime [ISO8601]', 'waterlevel_quality_controlled [m]']], on='datetime [ISO8601]')\n",
    "        sensor_data = sensor_data.merge(sensor_data_max[['datetime [ISO8601]', 'waterlevel_quality_controlled [m]']], on='datetime [ISO8601]')\n",
    "\n",
    "        # Rename columns\n",
    "        sensor_data.columns = ['NOS ID', 'datetime', 'mean', 'min', 'max']\n",
    "\n",
    "        # Plotting min, mean, and max\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.plot(sensor_data['datetime'], sensor_data['min'], label='Min')\n",
    "        ax.plot(sensor_data['datetime'], sensor_data['mean'], label='Mean')\n",
    "        ax.plot(sensor_data['datetime'], sensor_data['max'], label='Max')\n",
    "\n",
    "        # Running average\n",
    "        sensor_data['ra_min'] = sensor_data['min'].rolling(window=52).mean()\n",
    "        sensor_data['ra_mean'] = sensor_data['mean'].rolling(window=52).mean()\n",
    "        sensor_data['ra_max'] = sensor_data['max'].rolling(window=52).mean()\n",
    "\n",
    "        # Plotting running average\n",
    "        ax.plot(sensor_data['datetime'], sensor_data['ra_min'], 'r', label='Running average min')\n",
    "        ax.plot(sensor_data['datetime'], sensor_data['ra_mean'], 'r', label='Running average mean')\n",
    "        ax.plot(sensor_data['datetime'], sensor_data['ra_max'], 'r', label='Running average max')\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Water Level (m)')\n",
    "        ax.set_title(f'Tide Sensor {sensor} - {base_names_for_sensors[idx]}')\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "        \n",
    "        # Save the plot as an image file\n",
    "        fig.savefig('./plots/plot_' + str(sensor) + '.png')\n",
    "        \n",
    "        #plt.show()\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\conra\\anaconda3\\Lib\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\conra\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\conra\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Users\\conra\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m west_coast \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlaska\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalifornia\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHawaii\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOregon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWashington\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m west_coast_2  \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalifornia\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHawaii\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOregon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWashington\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m bases_df \u001b[38;5;241m=\u001b[39m data_import_bases(west_coast_2)\n",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m, in \u001b[0;36mdata_import_bases\u001b[1;34m(states)\u001b[0m\n\u001b[0;32m     37\u001b[0m bases_df \u001b[38;5;241m=\u001b[39m bases_df[bases_df\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39misin(states)]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# get elevation for each location\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# need to be improved with POST request\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m bases_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melevation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m bases_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeoPoint\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m point: get_elevation(point[\u001b[38;5;241m0\u001b[39m], point[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bases_df\n",
      "File \u001b[1;32mc:\\Users\\conra\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4759\u001b[0m         func,\n\u001b[0;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\Users\\conra\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\conra\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1291\u001b[0m )\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\conra\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\Users\\conra\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m, in \u001b[0;36mdata_import_bases.<locals>.<lambda>\u001b[1;34m(point)\u001b[0m\n\u001b[0;32m     37\u001b[0m bases_df \u001b[38;5;241m=\u001b[39m bases_df[bases_df\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39misin(states)]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# get elevation for each location\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# need to be improved with POST request\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m bases_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melevation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m bases_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeoPoint\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m point: get_elevation(point[\u001b[38;5;241m0\u001b[39m], point[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bases_df\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mget_elevation\u001b[1;34m(lat, lng)\u001b[0m\n\u001b[0;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.open-elevation.com/api/v1/lookup?locations=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlng\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melevation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\conra\\anaconda3\\Lib\\site-packages\\requests\\models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "west_coast = [\"Alaska\", \"California\", \"Guam\", \"Hawaii\", \"Oregon\", \"Washington\"]\n",
    "\n",
    "west_coast_2  = [\"California\", \"Hawaii\", \"Oregon\", \"Washington\"]\n",
    "\n",
    "bases_df = data_import_bases(west_coast_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1612480_20080101to20240106_qc.csv.gz', '9410170_20080101to20240128_qc.csv.gz.gz', '9410230_20080101to20240128_qc.csv.gz', '9413450_20080101to20240128_qc.csv.gz', '9447130_20080101to20240128_qc.csv.gz']\n"
     ]
    }
   ],
   "source": [
    "tide_sensors_df, tide_sensor_data_df = data_import_tidel_sensors() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_sensor_data_cleaned_df = curate_tide_sensor_data(tide_sensor_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_map(bases_df, sensors_df):\n",
    "\n",
    "sensors_df = tide_sensors_df\n",
    "\n",
    "\"\"\"\n",
    "Creates a map with folium.\n",
    "\n",
    "        Parameters:\n",
    "                    Bases   (DataFrame): Contains all US Bases that we want to see in the Map\n",
    "                    Sensors (DataFrame): Contains all Sensors that we want to see in the Map\n",
    "\n",
    "        Returns:\n",
    "                    None: \n",
    "\"\"\"\n",
    "\n",
    "# folium Map centered on Glasgow Hall\n",
    "nps_lat = 36.598802\n",
    "nps_lon = -121.877178\n",
    "\n",
    "# Create a map object\n",
    "# Specify center location, and starting zoom level (0 to 18)\n",
    "map = folium.Map(location=[nps_lat, nps_lon], zoom_start = 4, control_scal = True, tiles = \"Cartodb Positron\")\n",
    "\n",
    "# Add a marker for Glasgow Hall\n",
    "coord_list_bases = bases_df.geoPoint\n",
    "cod_list_sensors = sensors_df.geoPoint\n",
    "\n",
    "# Create popups\n",
    "popups_bases = ['<b>Base:</b><br>{}<br><b>Altitude:</b><br>{}'.format(name, elevation) for (name, elevation) in bases_df[['name', 'elevation']].values]\n",
    "popups_sensors = ['<b>Name:</b><br>{}'.format(name) for (name) in sensors_df['Location Name'].values]\n",
    "\n",
    "# Create a MarkerCluster object for bases\n",
    "marker_cluster_bases = MarkerCluster(\n",
    "    locations = coord_list_bases,\n",
    "    popups = popups_bases,\n",
    "    name='US Bases',\n",
    "    color='green',\n",
    "    overlay=True,\n",
    "    control=True\n",
    ")\n",
    "\n",
    "# Create a MarkerCluster object for sensors\n",
    "marker_cluster_sensors = MarkerCluster(\n",
    "    locations = cod_list_sensors,\n",
    "    popups = popups_sensors,\n",
    "    name='Tide Sensors',\n",
    "    color='blue',\n",
    "    overlay=True,\n",
    "    control=True\n",
    ")\n",
    "\n",
    "# Add MarkerCluster to map\n",
    "marker_cluster_bases.add_to(map)\n",
    "marker_cluster_sensors.add_to(map)\n",
    "\n",
    "folium.LayerControl().add_to(map)\n",
    "\n",
    "# Save the map    \n",
    "map.save('horbourTide.html')\n",
    "\n",
    "map  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_sensor_data_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_sensor_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_sensor_data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_sensor_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
